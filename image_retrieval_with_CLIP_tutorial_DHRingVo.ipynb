{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e29c71ac",
   "metadata": {},
   "source": [
    "# Image Retrieval with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0674656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import open_clip\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa02b3",
   "metadata": {},
   "source": [
    "## Download Image Dataset\n",
    "This notebook uses the image data and annotations provided by the FWF-funded project [Ottoman Nature in Travelogues, 1501â€“1850: A Digital Analysis (ONiT)](https://onit.oeaw.ac.at/). The images that we will be using were extracted from travelogues about the Ottoman Empire in English, French, German and Latin language that were printed between 1501 and 1850 and survived in the holdings of the [Austrian National Library (Ã–NB) in Vienna](https://www.onb.ac.at/). The images have been scanned in the course of the [Austrian Books Online](https://www.onb.ac.at/digitale-angebote/austrian-books-online) project â€“ a public private partnership between the Ã–NB and Google Books.\n",
    "\n",
    "1. Download the ZIP and CSV files from the following URL: https://1drv.ms/f/c/869f28ab041d44d9/ErTeV8fMekFCvtgexDOCxZoBCxCoxLJYUBOjam6rrwBSdw?e=IiKiVF\n",
    "2. Unzip the image data folder and save it in the Download folder\n",
    "3. Inspect the image data. Which types of images do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf850a5",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Load Data\n",
    "\n",
    "This cell loads the data from the downloaded CSV file. Filenames, paths, and other metadata of the 8720 images from the downloaded datasets are stored in lists. These will be used later to preview and load the images with the data loader and compute the image embeddings with CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "500d6d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786\n"
     ]
    }
   ],
   "source": [
    "# ONiT data paths\n",
    "onit_data_dir = '~/Downloads/curated_images_2024-11_hackathon_subset'\n",
    "onit_data = pd.read_csv('~/Downloads/ONiT_singleEd-images-curated_annotated_subset_2024-11.csv', sep=\",\", na_filter=False)\n",
    "\n",
    "# Load ONiT image data\n",
    "onit_filenames = []\n",
    "onit_paths = []\n",
    "lang_year = []\n",
    "labels = []\n",
    "\n",
    "# Load image paths\n",
    "for index, row in onit_data.iterrows():\n",
    "    onit_filename = row[\"filename\"]\n",
    "\n",
    "    if onit_filename[9] == \"_\":\n",
    "        barcode = onit_filename[:9]        \n",
    "    else:\n",
    "        barcode = onit_filename[:10]\n",
    "        \n",
    "    onit_tag = row[\"lang_year\"]\n",
    "    onit_path = os.path.join(onit_data_dir, onit_filename)\n",
    "    onit_path = os.path.expanduser(onit_path)\n",
    "    label = row[\"label_filled\"]\n",
    "    #print(type(label))\n",
    "    onit_paths.append(onit_path)\n",
    "    onit_filenames.append(onit_filename)\n",
    "    lang_year.append(onit_tag)\n",
    "    labels.append(label)\n",
    "\n",
    "print(len(onit_filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6184af1",
   "metadata": {},
   "source": [
    "### Inspect data\n",
    "In the Pandas dataframe below, inspect the loaded metadata for each image file.\n",
    "\n",
    "- **filename**: Filename of the corresponding image file.\n",
    "- **barcode**: Code of the physical book at the Austrian National Library (Ã–NB) from which the image was extracted.\n",
    "- **iiif**: Link to the extracted image on the Ã–NB IIIF server.\n",
    "- **lang_year**: Alphanumeric code describing language and century of the printed book (e.g. D16 means German, 16th century).\n",
    "- **label_filled**: Groundtruth with annotations of nature classes per image done in context of the ONiT project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "826d9e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>barcode</th>\n",
       "      <th>iiif</th>\n",
       "      <th>lang_year</th>\n",
       "      <th>label_filled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Z156577207_00009_page9_01.jpg</td>\n",
       "      <td>Z156577207</td>\n",
       "      <td>https://iiif.onb.ac.at/images/ABO/Z156577207/0...</td>\n",
       "      <td>D16</td>\n",
       "      <td>plants; vegetation, 25G*|animals, 25F*|birds, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z15958790X_00126_page126_01.jpg</td>\n",
       "      <td>Z15958790X</td>\n",
       "      <td>https://iiif.onb.ac.at/images/ABO/Z15958790X/0...</td>\n",
       "      <td>D16</td>\n",
       "      <td>landscapes, 25H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z160260500_00003_page3_01.jpg</td>\n",
       "      <td>Z160260500</td>\n",
       "      <td>https://iiif.onb.ac.at/images/ABO/Z160260500/0...</td>\n",
       "      <td>D16</td>\n",
       "      <td>animals, 25F|plants; vegetation, 25G*|fabulous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Z160260500_00033_page33_01.jpg</td>\n",
       "      <td>Z160260500</td>\n",
       "      <td>https://iiif.onb.ac.at/images/ABO/Z160260500/0...</td>\n",
       "      <td>D16</td>\n",
       "      <td>plants; vegetation, 25G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Z160260500_00229_page229_01.jpg</td>\n",
       "      <td>Z160260500</td>\n",
       "      <td>https://iiif.onb.ac.at/images/ABO/Z160260500/0...</td>\n",
       "      <td>D16</td>\n",
       "      <td>plants; vegetation, 25G*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>Z184478408_00111_page111_01.jpg</td>\n",
       "      <td>Z184478408</td>\n",
       "      <td>https://iiif.onb.ac.at/images/ABO/Z184478408/0...</td>\n",
       "      <td>L18</td>\n",
       "      <td>plants; vegetation, 25G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>Z184478408_00131_page131_01.jpg</td>\n",
       "      <td>Z184478408</td>\n",
       "      <td>https://iiif.onb.ac.at/images/ABO/Z184478408/0...</td>\n",
       "      <td>L18</td>\n",
       "      <td>plants; vegetation, 25G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>Z184478500_00075_page75_01.jpg</td>\n",
       "      <td>Z184478500</td>\n",
       "      <td>https://iiif.onb.ac.at/images/ABO/Z184478500/0...</td>\n",
       "      <td>L18</td>\n",
       "      <td>plants; vegetation, 25G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>Z184478500_00105_page105_01.jpg</td>\n",
       "      <td>Z184478500</td>\n",
       "      <td>https://iiif.onb.ac.at/images/ABO/Z184478500/0...</td>\n",
       "      <td>L18</td>\n",
       "      <td>plants; vegetation, 25G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>Z184478500_00189_page189_02.jpg</td>\n",
       "      <td>Z184478500</td>\n",
       "      <td>https://iiif.onb.ac.at/images/ABO/Z184478500/0...</td>\n",
       "      <td>L18</td>\n",
       "      <td>plants; vegetation, 25G</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>786 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            filename     barcode  \\\n",
       "0      Z156577207_00009_page9_01.jpg  Z156577207   \n",
       "1    Z15958790X_00126_page126_01.jpg  Z15958790X   \n",
       "2      Z160260500_00003_page3_01.jpg  Z160260500   \n",
       "3     Z160260500_00033_page33_01.jpg  Z160260500   \n",
       "4    Z160260500_00229_page229_01.jpg  Z160260500   \n",
       "..                               ...         ...   \n",
       "781  Z184478408_00111_page111_01.jpg  Z184478408   \n",
       "782  Z184478408_00131_page131_01.jpg  Z184478408   \n",
       "783   Z184478500_00075_page75_01.jpg  Z184478500   \n",
       "784  Z184478500_00105_page105_01.jpg  Z184478500   \n",
       "785  Z184478500_00189_page189_02.jpg  Z184478500   \n",
       "\n",
       "                                                  iiif lang_year  \\\n",
       "0    https://iiif.onb.ac.at/images/ABO/Z156577207/0...       D16   \n",
       "1    https://iiif.onb.ac.at/images/ABO/Z15958790X/0...       D16   \n",
       "2    https://iiif.onb.ac.at/images/ABO/Z160260500/0...       D16   \n",
       "3    https://iiif.onb.ac.at/images/ABO/Z160260500/0...       D16   \n",
       "4    https://iiif.onb.ac.at/images/ABO/Z160260500/0...       D16   \n",
       "..                                                 ...       ...   \n",
       "781  https://iiif.onb.ac.at/images/ABO/Z184478408/0...       L18   \n",
       "782  https://iiif.onb.ac.at/images/ABO/Z184478408/0...       L18   \n",
       "783  https://iiif.onb.ac.at/images/ABO/Z184478500/0...       L18   \n",
       "784  https://iiif.onb.ac.at/images/ABO/Z184478500/0...       L18   \n",
       "785  https://iiif.onb.ac.at/images/ABO/Z184478500/0...       L18   \n",
       "\n",
       "                                          label_filled  \n",
       "0    plants; vegetation, 25G*|animals, 25F*|birds, ...  \n",
       "1                                      landscapes, 25H  \n",
       "2    animals, 25F|plants; vegetation, 25G*|fabulous...  \n",
       "3                              plants; vegetation, 25G  \n",
       "4                             plants; vegetation, 25G*  \n",
       "..                                                 ...  \n",
       "781                            plants; vegetation, 25G  \n",
       "782                            plants; vegetation, 25G  \n",
       "783                            plants; vegetation, 25G  \n",
       "784                            plants; vegetation, 25G  \n",
       "785                            plants; vegetation, 25G  \n",
       "\n",
       "[786 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect loaded data - the CSV contains the metadata for each image file.\n",
    "onit_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27457f26",
   "metadata": {},
   "source": [
    "### Load image preprocessing parameters and data loader\n",
    "\n",
    "Image preprocessing parameters are necessary to load the images in the correct size and ratio into the CLIP model. The pad function keeps the aspect ratio of the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff9a0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import image preprocessing packages\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Define custom image preprocessing parameters\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 224\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "#train_val_split_ratio = 0.8 # 80/20 ratio for training/validation data\n",
    "\n",
    "# Define pad function (add black borders to keep aspect ratio)\n",
    "class SquarePad:\n",
    "    def __call__(self, image):\n",
    "        w, h = image.size\n",
    "        max_wh = np.max([w, h])\n",
    "        hp = int((max_wh - w) / 2)\n",
    "        vp = int((max_wh - h) / 2)\n",
    "        padding = (hp, vp, hp, vp)\n",
    "        return F.pad(image, padding, 0, 'constant')\n",
    "\n",
    "# Define custom preprocess function\n",
    "preprocess = transforms.Compose([\n",
    "    SquarePad(),\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de04f2fa",
   "metadata": {},
   "source": [
    "Dataset class and data loader are defined below to correctly load the images into the CLIP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7dadec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dataset class & Data Loader\n",
    "class OnitFull(Dataset):\n",
    "    def __init__(self, onit_paths, transform=None):\n",
    "        self.onit_paths = onit_paths\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.onit_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.onit_paths[idx]\n",
    "        try:\n",
    "            with open(image_path, 'rb') as f:\n",
    "                image = Image.open(f).convert(\"RGB\")\n",
    "                image_prep = preprocess(image)\n",
    "                return image_prep\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Image file {image_path} not found. Skipping...\")\n",
    "            pass\n",
    "\n",
    "# Set Data Loader\n",
    "onit_full_dataset = OnitFull(onit_paths)\n",
    "onitdata_loader = DataLoader(onit_full_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0623947",
   "metadata": {},
   "source": [
    "## Load model and get features\n",
    "\n",
    "1. Go to Huggingface and select the model that you want to use. In this example, we are going to use the CLIP-ViT-B-32 model pretrained on the [LAION-2B English subset of LAION-5B](https://laion.ai/blog/laion-5b/) dataset. You can find the model here: https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K\n",
    "2. Click on the button \"Use this model\" to copy the code for loading the model via Huggingface in your Python script. You can also directly open it in a Google Collab or Kaggle notebook.\n",
    "3. In the cell below, the code to load the CLIP model is already inserted. Before loading the model, make sure to set the device (i.e. if the model is run on GPU or CPU). The code below automatically checks if a GPU is available. If a GPU is available, the device is automatically set to CUDA. Otherwise the CPU is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "556914fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "## https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K?library=open_clip\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: \", device)\n",
    "\n",
    "# Load model\n",
    "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:laion/CLIP-ViT-B-32-laion2B-s34B-b79K', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376ac4c",
   "metadata": {},
   "source": [
    "4. The get features function below loads the images with the data loader defined above in batches and embeds all images. Depending on the device used, this can take several minutes.\n",
    "5. Once the embeddings are calculated, store them in a numpy file so that you can load them later without having to calculate the features again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df586dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14687442ff874083b81d333e08686a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get features of ONiT dataset with loaded image encoder of fine-tuned/pre-trained model\n",
    "\n",
    "# Get Features function\n",
    "def get_features(onit_full_dataset):\n",
    "    image_features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image_prep in tqdm(onitdata_loader):\n",
    "            #print('Images: ', image_prep.shape)\n",
    "            imfeat = model.encode_image(image_prep.to(device)) ## Pre-trained model\n",
    "            \n",
    "            image_features.append(imfeat)\n",
    "\n",
    "    return torch.cat(image_features).cpu().numpy()\n",
    "\n",
    "# Calculate image features\n",
    "image_features = get_features(onit_full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ad726ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the embeddings\n",
    "np.save(\"onit_images_CURATED_embeddings_openCLIP.npy\", image_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6c7baa",
   "metadata": {},
   "source": [
    "# Image retrieval with natural language prompt\n",
    "## Load tokenizer and image vectors\n",
    "\n",
    "Once the image embeddings are calculated, we can start with the image retrieval using vectors. Before that, we have to set the correct tokenizer for the natural language prompt and load our embeddings (or image features) from the numpy file.\n",
    "\n",
    "To find similar vectors, we will use cosine similarity. We will import it from the Scikit Learn library.\n",
    "\n",
    "1. Load the tokenizer to preprocess the natural language prompts before passing them to the language transformer.\n",
    "2. Load the image embeddings and scikit learn cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdbdd912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import text preprocessing packages\n",
    "from open_clip import tokenizer\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('hf-hub:laion/CLIP-ViT-B-32-laion2B-s34B-b79K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2631d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786\n",
      "786\n"
     ]
    }
   ],
   "source": [
    "# Load image features\n",
    "imagesCurated = \"onit_images_CURATED_embeddings_openCLIP.npy\"\n",
    "\n",
    "loaded_image_features = np.load(imagesCurated)\n",
    "image_embeddings = torch.tensor(loaded_image_features, device=device)\n",
    "\n",
    "print(len(image_embeddings))\n",
    "print(len(onit_paths))\n",
    "\n",
    "# Import image similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee743d7d",
   "metadata": {},
   "source": [
    "## Retrieval based on cosine similarity\n",
    "\n",
    "Now that we have set our prompt, we need to pass it to CLIP's language transformer.\n",
    "\n",
    "1. First, the text prompt is tokenized and encoded with the language transformer. Since it is just one text vector, this is really fast.\n",
    "2. In the next step, the cosine similarity is calculated between the text embedding and all image embeddings previously computed/loaded.\n",
    "3. To find the similar indices, the indices are sorted by the resulting similarities. A ranking with the highest similarity scores on top is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "660b293b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embedding shape [number of vectors, dimensions]:  torch.Size([1, 512])\n",
      "Image embeddings shape [number of vectors, dimensions]:  torch.Size([786, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example text prompt\n",
    "text_prompt = \"An image of a horse\" ## GT: \"horses and kindred animals, 46C1314\"\n",
    "\n",
    "# Encode the text prompt to get the text embedding\n",
    "with torch.no_grad():\n",
    "    text_embedding = model.encode_text(tokenizer(text_prompt).to(device)) #for original CLIP model\n",
    "\n",
    "print(\"Text embedding shape [number of vectors, dimensions]: \", text_embedding.shape)\n",
    "print(\"Image embeddings shape [number of vectors, dimensions]: \", image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef8699b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cosine similarity between the text prompt and all images\n",
    "similarities = cosine_similarity(text_embedding.cpu(), image_embeddings.cpu())\n",
    "similarities_tensor = torch.tensor(similarities[0], device=device)\n",
    "\n",
    "# Sort the images based on similarity (descending order)\n",
    "similar_images_indices = torch.argsort(similarities_tensor, descending=True)\n",
    "similar_ranks = list(enumerate(similar_images_indices.tolist()))\n",
    "#print(similar_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c771ae9",
   "metadata": {},
   "source": [
    "## Preview retrieval results\n",
    "\n",
    "Finally, we can try our own vector similarity search!\n",
    "\n",
    "**The following cell creates a simple interface to inspect the first N retrieved images. You can inspect the rank, similarity score, filename, and language/century of each image retrieved.** In the search bar below, enter a text prompt describing the type of images that you want to look for and press the search button. The model works best in English, but also German prompts are able to yield good results.\n",
    "\n",
    "Like in the code above, the string is passed to the language transformer and vectorized before calculating the cosine similarity. The resulting ranking sorts the image indices by similarity, as displayed in the code above. We can now inspect the results to check which images are the most similar to our text prompt according to the CLIP model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "430e48c2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218a6a7b655d47d7b27bff9490b8fc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='An image of a dog', description='Prompt:', layout=Layout(width='70%'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Run to start interactive CLIP image search & preview of top N results ##\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# --- UI widgets ---\n",
    "text_box = widgets.Text(\n",
    "    value=\"An image of a dog\",\n",
    "    description=\"Prompt:\",\n",
    "    placeholder=\"Enter text prompt\",\n",
    "    layout=widgets.Layout(width='70%')\n",
    ")\n",
    "top_n_slider = widgets.IntSlider(\n",
    "    value=36,\n",
    "    min=4, max=100, step=4,\n",
    "    description='Top N:',\n",
    "    continuous_update=False,\n",
    "    style={'description_width': '70px'}\n",
    ")\n",
    "search_button = widgets.Button(\n",
    "    description=\"Search\",\n",
    "    button_style='success',\n",
    "    tooltip=\"Search for similar images\",\n",
    "    icon='search'\n",
    ")\n",
    "output = widgets.Output()\n",
    "\n",
    "# --- main search function ---\n",
    "def on_search_clicked(b):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        text_prompt = text_box.value.strip()\n",
    "        if not text_prompt:\n",
    "            print(\"âš ï¸ Please enter a text prompt.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Searching for images similar to: '{text_prompt}'\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_embedding = model.encode_text(tokenizer(text_prompt).to(device))\n",
    "\n",
    "        similarities = cosine_similarity(text_embedding.cpu(), image_embeddings.cpu())\n",
    "        similarities_tensor = torch.tensor(similarities[0], device=device)\n",
    "\n",
    "        similar_images_indices = torch.argsort(similarities_tensor, descending=True)\n",
    "        top_n = top_n_slider.value\n",
    "\n",
    "        # --- visualization ---\n",
    "        cols = 6\n",
    "        rows = (top_n + cols - 1) // cols\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(12, 2 * rows))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i in range(top_n):\n",
    "            index = similar_images_indices[i].item()\n",
    "            img = Image.open(onit_paths[index])\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis(\"off\")\n",
    "            axes[i].set_title(\n",
    "                f\"Rank: {i+1}\\nSim: {similarities_tensor[index]:.4f}\\n{onit_filenames[index][:25]}\\n{lang_year[index]}\",\n",
    "                fontsize=8\n",
    "            )\n",
    "\n",
    "        # hide unused axes\n",
    "        for j in range(top_n, len(axes)):\n",
    "            axes[j].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# --- bind button ---\n",
    "search_button.on_click(on_search_clicked)\n",
    "\n",
    "# --- layout ---\n",
    "ui = widgets.VBox([\n",
    "    widgets.HBox([text_box, top_n_slider, search_button]),\n",
    "    output\n",
    "])\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855c39a",
   "metadata": {},
   "source": [
    "# Check against Ground Truth\n",
    "\n",
    "How accurate are the results that we achieved? To get a better idea about the accuracy and performance of CLIP, we can compare the retrieval results with the annotated ground truth. Remember that the text vector is only compared to the image vectors for the similarity search. The manually annotated ground truth is used to compare how well the retrieval results match the human classification.\n",
    "\n",
    "**Run the cell below to compare the annotated ground truth (GT) with the search results.**\n",
    "\n",
    "## How well did the model perform?\n",
    "\n",
    "**The cell below calculates the ratio of true positives (TP) and true negatives (TN) that the model could retrieve for the first k results.**\n",
    "\n",
    "For example, of the first 30 retrieved images for the text prompt \"image of a camel\", only 2 are actually camels (=TP), i.e. 6,67% of the retrieved images.\n",
    "\n",
    "Let's calculate some more metrics. **To analyse the relationships of true positives, false positives, and false negatives, we calculate recall at k and precision at k.**\n",
    "\n",
    "- **Recall at k** measures the percentage of relevant items found in the top K retrieved results out of all relevant items (i.e. total examples per class).\n",
    "- **Precision at k** measures the percentage of relevant items within the top K retrieved results (which corresponds to the percentage of TP in the first k retrieved results that we calculated before).\n",
    "\n",
    "An additional measure to examine the modelâ€™s capability to retrieve images relevant to the query is **R-precision**. It can be useful to assess how well the model performs for a large number of relevant documents. For example, we know that we have 24 camels in our dataset. Therefore, a perfect system would retrieve 100% camels in the top 24 ranked results.\n",
    "\n",
    "- **R-precision** shows the percentage of images from our Gold Standard that the model retrieved for the first R-similarities, where R is the total number of examples per class in our image dataset.\n",
    "\n",
    "In our case, R-precision for dogs in our dataset is only 5.5%. What does this mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dc03beb",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ab57c824e2469aaf4bc4b34590770f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Select Keyword:', layout=Layout(width='60%'), options=('anâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Run to start interactive inspection of TP, FP and metrics ##\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# --- Map keywords to default prompts ---\n",
    "keyword_prompt_map = {\n",
    "    \"animals, 25F\": \"image of an animal\",\n",
    "    \"plants; vegetation, 25G\": \"image of a plant\",\n",
    "    \"landscapes, 25H\": \"image of a landscape\",\n",
    "    \"maps; atlases, 25A\": \"image of a map\",\n",
    "    \"horses and kindred animals, 46C1314\": \"image of a horse\",\n",
    "    \"birds, 25F3\": \"image of a bird\",\n",
    "    \"camel, 25F24\": \"image of a camel\",\n",
    "    \"hoofed animals (GIRAFFE), 25F24\": \"image of a giraffe\"\n",
    "}\n",
    "\n",
    "# --- UI widgets ---\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=list(keyword_prompt_map.keys()),\n",
    "    value=\"animals, 25F\",\n",
    "    description=\"Select Keyword:\",\n",
    "    layout=widgets.Layout(width=\"60%\")\n",
    ")\n",
    "top_n_slider = widgets.IntSlider(\n",
    "    value=30,\n",
    "    min=10, max=200, step=10,\n",
    "    description=\"Top-N:\",\n",
    "    continuous_update=False\n",
    ")\n",
    "run_button = widgets.Button(\n",
    "    description=\"Search\",\n",
    "    button_style=\"info\",\n",
    "    icon=\"search\"\n",
    ")\n",
    "output = widgets.Output()\n",
    "\n",
    "# --- Main search and evaluation function ---\n",
    "def on_search_clicked(b):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        selected_keyword = dropdown.value\n",
    "        text_prompt = keyword_prompt_map[selected_keyword]\n",
    "        k = top_n_slider.value\n",
    "\n",
    "        print(f\"ðŸ” Searching for images similar to: '{text_prompt}'\")\n",
    "        print(f\"âœ… Using keyword for TP/FP: '{selected_keyword}'\")\n",
    "        print(f\"Top-{k} images\\n\")\n",
    "\n",
    "        # --- Encode text prompt ---\n",
    "        with torch.no_grad():\n",
    "            text_embedding = model.encode_text(tokenizer(text_prompt).to(device))\n",
    "\n",
    "        # --- Compute similarity ---\n",
    "        similarities = cosine_similarity(text_embedding.cpu(), image_embeddings.cpu())\n",
    "        similarities_tensor = torch.tensor(similarities[0], device=device)\n",
    "        similar_images_indices = torch.argsort(similarities_tensor, descending=True)\n",
    "        similar_images_indices_list = similar_images_indices[:k].cpu().numpy().tolist()\n",
    "        similar_images_indices_full = similar_images_indices.cpu().numpy().tolist()\n",
    "\n",
    "        # --- TP / FP analysis ---\n",
    "        indices_TP, indices_FP = [], []\n",
    "        total_tuples = 0\n",
    "        for idx in similar_images_indices_list:\n",
    "            label_tuple = tuple(labels[idx].split(\"|\"))\n",
    "            if isinstance(label_tuple, tuple):\n",
    "                total_tuples += 1\n",
    "                if any(selected_keyword in item for item in label_tuple):\n",
    "                    indices_TP.append(idx)\n",
    "                else:\n",
    "                    indices_FP.append(idx)\n",
    "\n",
    "        percentage_TP = (len(indices_TP)/total_tuples*100) if total_tuples else 0\n",
    "        #print(f\"Total tuples checked: {total_tuples}\")\n",
    "        print(f\"âœ… True Positives: {len(indices_TP)} | âŒ False Positives: {len(indices_FP)}\")\n",
    "        print(f\"ðŸ“Š Percentage TP: {percentage_TP:.2f}%\\n\")\n",
    "\n",
    "        # --- FN / TN / Precision / Recall ---\n",
    "        indices_labelled_animals = []\n",
    "        similarities_scores_animals = []\n",
    "        for idx in similar_images_indices_full:\n",
    "            label_tuple = tuple(labels[idx].split(\"|\"))\n",
    "            if isinstance(label_tuple, tuple) and any(selected_keyword in item for item in label_tuple):\n",
    "                indices_labelled_animals.append(idx)\n",
    "                similarities_scores_animals.append(similarities_tensor[idx])\n",
    "\n",
    "        # False Negatives (FN)\n",
    "        indices_FN = [idx for idx in indices_labelled_animals if idx not in indices_TP]\n",
    "\n",
    "        # True Negatives (TN)\n",
    "        TN = len(similar_images_indices_list) - len(indices_labelled_animals) - len(indices_FP)\n",
    "\n",
    "        # Recall@K\n",
    "        recall_at_k = len(indices_TP) / (len(indices_TP) + len(indices_FN)) if (len(indices_TP) + len(indices_FN)) else 0\n",
    "\n",
    "        # Precision@K\n",
    "        precision_at_k = len(indices_TP) / (len(indices_TP) + len(indices_FP)) if (len(indices_TP) + len(indices_FP)) else 0\n",
    "\n",
    "        print(f\"ðŸ–¼ï¸ Total occurrences of '{selected_keyword}': {len(indices_labelled_animals)}\")\n",
    "        print(f\"âš ï¸ False Negatives: {len(indices_FN)}\")\n",
    "        print(f\"Recall@{k}: {recall_at_k:.3f}\")\n",
    "        print(f\"Precision@{k}: {precision_at_k:.3f}\")\n",
    "\n",
    "        # --- R-precision ---\n",
    "        R = len(indices_labelled_animals)\n",
    "        indices_rel = []\n",
    "        similarities_scores_TP = []\n",
    "\n",
    "        for index in similar_images_indices_list[:R]:\n",
    "            label_tuple = tuple(labels[index].split(\"|\"))\n",
    "            if isinstance(label_tuple, tuple) and any(selected_keyword in item for item in label_tuple):\n",
    "                indices_rel.append(index)\n",
    "                similarities_scores_TP.append(similarities_tensor[index])\n",
    "\n",
    "        R_precision = len(indices_rel)/R if R > 0 else 0\n",
    "        print(f\"R-precision at {R}: {R_precision:.3f}\\n\")\n",
    "\n",
    "        # --- Visualization with colored rectangles ---\n",
    "        cols = 10\n",
    "        rows = (k + cols - 1) // cols\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(18, 2.5*rows))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, idx in enumerate(similar_images_indices_list):\n",
    "            img = Image.open(onit_paths[idx])\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis(\"off\")\n",
    "\n",
    "            # Draw TP/FP rectangle\n",
    "            if idx in indices_TP:\n",
    "                rect = patches.Rectangle(\n",
    "                    (0, 0), img.size[0], img.size[1],\n",
    "                    linewidth=4, edgecolor='green', facecolor='none'\n",
    "                )\n",
    "                axes[i].add_patch(rect)\n",
    "            elif idx in indices_FP:\n",
    "                rect = patches.Rectangle(\n",
    "                    (0, 0), img.size[0], img.size[1],\n",
    "                    linewidth=4, edgecolor='red', facecolor='none'\n",
    "                )\n",
    "                axes[i].add_patch(rect)\n",
    "\n",
    "            sim_score = similarities_tensor[idx].item()\n",
    "            axes[i].set_title(\n",
    "                f\"{i+1}. {onit_filenames[idx][:15]}\\n{lang_year[idx]}\\n{sim_score:.3f}\",\n",
    "                fontsize=8\n",
    "            )\n",
    "\n",
    "        for j in range(k, len(axes)):\n",
    "            axes[j].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# --- Bind button ---\n",
    "run_button.on_click(on_search_clicked)\n",
    "\n",
    "# --- Display UI ---\n",
    "ui = widgets.VBox([\n",
    "    widgets.HBox([dropdown, top_n_slider, run_button]),\n",
    "    output\n",
    "])\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f5d10",
   "metadata": {},
   "source": [
    "**Done!** You have now learned how to load a CLIP model from Huggingface, how to prepare and load your data into the model, how to embed the images and text prompt, and how to use cosine similarity to rank the images according to their similarity with the vectorized text prompt.\n",
    "You have also learned how to inspect the results, assess how well the model performs by calculating precision/recall metrics, and understand the limitations of the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d4eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLIP-tutorial",
   "language": "python",
   "name": "clip-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
